{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hzm4U-TgmQ_u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "046cdb8d-e505-4d28-b413-83c952184b2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nouvelle section"
      ],
      "metadata": {
        "id": "UYdqA4-C-QJa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transform MindMap generated by xmind to json\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YNVVBoBDzDqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install xmindparser\n",
        "\n",
        "import xmindparser\n",
        "import json\n",
        "\n",
        "def transform_xmind_to_json(xmind_file_path, json_save_path):\n",
        "  \"\"\"\n",
        "  Transforms an XMind mind map file into a JSON format.\n",
        "\n",
        "  Args:\n",
        "    xmind_file_path (str): The absolute or relative path to the .xmind file.\n",
        "    json_save_path (str): The absolute or relative path where the JSON file will be saved.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    # Parse the xmind file\n",
        "    data = xmindparser.xmind_to_dict(xmind_file_path)\n",
        "\n",
        "    # Save the parsed data to a JSON file\n",
        "    with open(json_save_path, 'w', encoding='utf-8') as f:\n",
        "      json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"Mind map successfully transformed from '{xmind_file_path}' to '{json_save_path}'\")\n",
        "\n",
        "  except FileNotFoundError:\n",
        "    print(f\"Error: The file '{xmind_file_path}' was not found.\")\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "# --- Instructions on where to put your file paths ---\n",
        "\n",
        "# 1. Replace 'path/to/your/mindmap.xmind' with the actual path to your XMind file.\n",
        "#    - If the file is in the same directory as your notebook, just use the filename (e.g., 'my_mindmap.xmind').\n",
        "#    - If the file is in a different directory, provide the full path (e.g., '/content/drive/MyDrive/my_mindmaps/project_plan.xmind').\n",
        "\n",
        "# 2. Replace 'path/to/save/output.json' with the desired path and filename for your output JSON file.\n",
        "#    - If you want to save it in the same directory, just use the filename (e.g., 'output_mindmap.json').\n",
        "#    - If you want to save it in a specific directory, provide the full path (e.g., '/content/processed_json/project_plan.json').\n",
        "\n",
        "# Example Usage:\n",
        "xmind_file = '/content/sample_data/MM_RAG/Offboarding Expert MM.xmind' # <--- Put your XMind file path here\n",
        "json_file = '/content/sample_data/MM_RAG/off_boarding_MM.json'   # <--- Put your desired JSON save path here\n",
        "\n",
        "transform_xmind_to_json(xmind_file, json_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCRvioNqyuyi",
        "outputId": "f5606979-8528-4f87-f544-a1f00672adc4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xmindparser in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Mind map successfully transformed from '/content/sample_data/MM_RAG/Offboarding Expert MM.xmind' to '/content/sample_data/MM_RAG/off_boarding_MM.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XMjeZsJh7FLN",
        "outputId": "b428ae73-844d-4854-ab9f-f3b6e00470ba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb\n",
            "  Downloading chromadb-1.0.12-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.5)\n",
            "Collecting fastapi==0.115.9 (from chromadb)\n",
            "  Downloading fastapi-0.115.9-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.3)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.0.2)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.0.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.14.0)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.55b1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.73.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.16.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.1.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.18)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.24.0)\n",
            "Collecting starlette<0.46.0,>=0.40.0 (from fastapi==0.115.9->chromadb)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.25.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.4.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.55b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.55b1-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting opentelemetry-instrumentation==0.55b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.55b1-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-util-http==0.55b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.55b1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.55b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.55b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.33.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.3)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading chromadb-1.0.12-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.9-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.34.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.55b1-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.55b1-py3-none-any.whl (31 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.55b1-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl (196 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_util_http-0.55b1-py3-none-any.whl (7.3 kB)\n",
            "Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-5.0.0-py3-none-any.whl (100 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.0/100.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=d6a91e3e1c1b85d4a1b77eb09dd81a391103547eb1473d24f6a499ff2050f172\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, python-dotenv, overrides, opentelemetry-util-http, opentelemetry-proto, mmh3, humanfriendly, httptools, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, opentelemetry-semantic-conventions, onnxruntime, kubernetes, fastapi, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
            "  Attempting uninstall: starlette\n",
            "    Found existing installation: starlette 0.46.2\n",
            "    Uninstalling starlette-0.46.2:\n",
            "      Successfully uninstalled starlette-0.46.2\n",
            "  Attempting uninstall: fastapi\n",
            "    Found existing installation: fastapi 0.115.12\n",
            "    Uninstalling fastapi-0.115.12:\n",
            "      Successfully uninstalled fastapi-0.115.12\n",
            "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 chromadb-1.0.12 coloredlogs-15.0.1 durationpy-0.10 fastapi-0.115.9 httptools-0.6.4 humanfriendly-10.0 kubernetes-33.1.0 mmh3-5.1.0 onnxruntime-1.22.0 opentelemetry-api-1.34.1 opentelemetry-exporter-otlp-proto-common-1.34.1 opentelemetry-exporter-otlp-proto-grpc-1.34.1 opentelemetry-instrumentation-0.55b1 opentelemetry-instrumentation-asgi-0.55b1 opentelemetry-instrumentation-fastapi-0.55b1 opentelemetry-proto-1.34.1 opentelemetry-sdk-1.34.1 opentelemetry-semantic-conventions-0.55b1 opentelemetry-util-http-0.55b1 overrides-7.7.0 posthog-5.0.0 pypika-0.48.9 python-dotenv-1.1.0 starlette-0.45.3 uvloop-0.21.0 watchfiles-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets accelerate bitsandbytes peft trl xmindparser PyMuPDF langchain-google-genai python-dotenv chromadb"
      ],
      "metadata": {
        "id": "eViedMCe7n80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c7b37ed-8476-4e0e-f67f-6d062b15b7c0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m366.4/366.4 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yHVYog3u75QI",
        "outputId": "ef41bf52-c8f8-4cde-af12-f996bd3b3c5c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.65)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.5)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HZNOO7IJ8Pd6",
        "outputId": "0b59c863-3066-4adc-ef7d-f587e02eb401"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.25-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.65 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.65)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.25 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.25)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.45)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (2.11.5)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain-community) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.65->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.25-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.25 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from typing import List, Dict, Any, Tuple\n",
        "\n",
        "# --- Configuration (identique au précédent, assurez-vous des clés API) ---\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"\"\n",
        "\n",
        "import openai\n",
        "import google.generativeai as genai\n",
        "import chromadb\n",
        "from langchain_community.vectorstores import Chroma # Import Chroma here\n",
        "\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
        "\n",
        "MINDMAP_JSON_PATH = \"/content/sample_data/MM_RAG/off_boarding_MM.json\"\n",
        "CHROMA_DB_PATH = \"/content/sample_data/MM_RAG/db\"\n",
        "client_chroma = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
        "# The collection name '/content/sample_data/MM_RAG/Documents' looks like a path.\n",
        "# ChromaDB collection names should be simple strings, usually alphanumeric with hyphens or underscores.\n",
        "# Let's assume you intended the collection name to be something like 'Documents' or 'MM_RAG_Docs'.\n",
        "# If you created your collection with a different name, replace 'Documents' below.\n",
        "COLLECTION_NAME = \"/content/sample_data/MM_RAG/Documents\" # <--- Corrected Collection Name\n",
        "\n",
        "try:\n",
        "    # Attempt to get the collection. If it doesn't exist, this will raise an exception.\n",
        "    collection = client_chroma.get_collection(name=COLLECTION_NAME)\n",
        "    print(f\"Collection ChromaDB '{COLLECTION_NAME}' chargée.\")\n",
        "except Exception as e:\n",
        "    print(f\"Erreur lors du chargement de la collection ChromaDB '{COLLECTION_NAME}': {e}\")\n",
        "    print(\"Assurez-vous que la collection existe et que le chemin est correct.\")\n",
        "    # Depending on your workflow, you might want to create the collection here\n",
        "    # if it's missing, or handle the case where it doesn't exist.\n",
        "    # For now, we just print an error.\n",
        "    collection = None # Set collection to None if loading fails\n",
        "\n",
        "\n",
        "mindmap_data = {}\n",
        "if os.path.exists(MINDMAP_JSON_PATH):\n",
        "    with open(MINDMAP_JSON_PATH, 'r', encoding='utf-8') as f:\n",
        "        mindmap_data = json.load(f)\n",
        "    print(f\"Mind map chargée depuis {MINDMAP_JSON_PATH}\")\n",
        "else:\n",
        "    print(f\"ATTENTION : Fichier de mind map non trouvé à {MINDMAP_JSON_PATH}. Le système fonctionnera sans données de mind map.\")\n",
        "\n",
        "\n",
        "# --- 1. Fonctions de Récupération Améliorées ---\n",
        "\n",
        "def retrieve_from_chromadb(query: str, top_k: int = 5, document_ids_filter: List[str] = None) -> List[str]:\n",
        "    \"\"\"\n",
        "    Récupère les chunks les plus pertinents de ChromaDB.\n",
        "    Permet de filtrer par une liste d'IDs de documents.\n",
        "    \"\"\"\n",
        "    # Check if the collection object was successfully loaded\n",
        "    if collection is None:\n",
        "        print(\"ChromaDB collection is not loaded, cannot retrieve documents.\")\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        where_clause = None\n",
        "        if document_ids_filter:\n",
        "            # Assurez-vous que vos métadonnées dans ChromaDB contiennent un champ pour l'ID du document source\n",
        "            # Par exemple, si vous avez stocké l'ID du document original sous la clé 'document_id'\n",
        "            # Note: The metadata key 'document_id' should match what you used during ingestion.\n",
        "            # If you used 'source' as shown in the ingestion code, change 'document_id' to 'source'.\n",
        "            # Let's assume 'source' based on the ingestion example.\n",
        "            where_clause = {\"source\": {\"$in\": document_ids_filter}}\n",
        "            print(f\"Recherche dans ChromaDB avec filtre de sources : {document_ids_filter}\")\n",
        "\n",
        "        # Ensure the query is a list as expected by collection.query\n",
        "        results = collection.query(\n",
        "            query_texts=[query],\n",
        "            n_results=top_k,\n",
        "            where=where_clause, # Applique le filtre ici\n",
        "            include=['documents', 'metadatas']\n",
        "        )\n",
        "\n",
        "        if results and results.get('documents') and results['documents'][0]:\n",
        "            # Return the list of documents (chunks)\n",
        "            return results['documents'][0]\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur lors de la recherche dans ChromaDB : {e}\")\n",
        "        return []\n",
        "\n",
        "def extract_info_and_links_from_mindmap(query: str, mindmap_json: Dict[str, Any]) -> Tuple[List[str], List[str]]:\n",
        "    \"\"\"\n",
        "    Extrait les informations textuelles pertinentes de la mind map ET les liens vers les documents.\n",
        "    Retourne un tuple: (informations_texte_mindmap, liste_ids_documents_liés).\n",
        "\n",
        "    Assurez-vous que votre structure JSON de mind map inclut une clé pour les liens, par exemple 'linked_docs'.\n",
        "    Exemple de structure de nœud mind map attendue:\n",
        "    {\n",
        "      \"id\": \"node_id\",\n",
        "      \"text\": \"Titre du Nœud\",\n",
        "      \"details\": \"Description du nœud.\",\n",
        "      \"linked_docs\": [\"doc_id_1\", \"doc_id_2\"] # Liste d'IDs de documents dans votre système\n",
        "    }\n",
        "    Note: 'linked_docs' values should match the 'source' metadata you used in ChromaDB.\n",
        "    \"\"\"\n",
        "    relevant_text_info = []\n",
        "    relevant_doc_ids = []\n",
        "\n",
        "    # Check if mindmap_json is a dictionary before accessing \"nodes\"\n",
        "    if isinstance(mindmap_json, dict) and \"nodes\" in mindmap_json:\n",
        "        for node in mindmap_json[\"nodes\"]:\n",
        "            node_text = node.get(\"text\", \"\").lower()\n",
        "            node_details = node.get(\"details\", \"\").lower()\n",
        "            query_lower = query.lower()\n",
        "\n",
        "            # Simple correspondance de mots-clés pour trouver des nœuds pertinents\n",
        "            # Vous pouvez améliorer cela avec des techniques de NLP ou des embeddings si nécessaire\n",
        "            if query_lower in node_text or query_lower in node_details:\n",
        "                relevant_text_info.append(f\"Mind Map Node: {node.get('text', '')} - Details: {node.get('details', '')}\")\n",
        "\n",
        "                # Si le nœud pertinent a des liens vers des documents, ajoutez-les\n",
        "                if \"linked_docs\" in node and isinstance(node[\"linked_docs\"], list):\n",
        "                    relevant_doc_ids.extend(node[\"linked_docs\"])\n",
        "\n",
        "    # Éliminer les doublons d'IDs de documents\n",
        "    relevant_doc_ids = list(set(relevant_doc_ids))\n",
        "\n",
        "    return relevant_text_info, relevant_doc_ids\n",
        "\n",
        "# --- 2. Fonctions de Génération et Prompt (identiques au précédent) ---\n",
        "\n",
        "def build_rag_prompt(user_query: str, retrieved_docs: List[str], mindmap_info: List[str]) -> str:\n",
        "    \"\"\"Construit le prompt pour le modèle de langage.\"\"\"\n",
        "    prompt_parts = [\n",
        "        \"Vous êtes un assistant IA qui fournit des réponses basées sur les informations fournies.\",\n",
        "        \"Voici la question de l'utilisateur:\",\n",
        "        f\"Question: {user_query}\\n\",\n",
        "    ]\n",
        "\n",
        "    if retrieved_docs:\n",
        "        prompt_parts.append(\"Voici des extraits de documents pertinents:\")\n",
        "        # Ensure retrieved_docs is a list of strings\n",
        "        for i, doc in enumerate(retrieved_docs):\n",
        "             # If 'doc' is not a string (e.g., a Document object), access the text content\n",
        "             doc_content = doc if isinstance(doc, str) else getattr(doc, 'page_content', str(doc))\n",
        "             prompt_parts.append(f\"Document {i+1}: {doc_content}\")\n",
        "        prompt_parts.append(\"\")\n",
        "\n",
        "    if mindmap_info:\n",
        "        prompt_parts.append(\"Voici des informations pertinentes extraites de la mind map:\")\n",
        "        for i, info in enumerate(mindmap_info):\n",
        "            prompt_parts.append(f\"Mind Map Info {i+1}: {info}\")\n",
        "        prompt_parts.append(\"\")\n",
        "\n",
        "    prompt_parts.append(\"En utilisant uniquement les informations fournies (extraits de documents et de la mind map), répondez à la question de l'utilisateur de manière complète et concise. Si les informations ne sont pas suffisantes, indiquez-le.\")\n",
        "\n",
        "    return \"\\n\".join(prompt_parts)\n",
        "\n",
        "def generate_with_openai(prompt: str, model: str = \"gpt-3.5-turbo\") -> str:\n",
        "    \"\"\"Génère une réponse en utilisant l'API OpenAI.\"\"\"\n",
        "    try:\n",
        "        # Ensure openai.api_key is set\n",
        "        if not openai.api_key:\n",
        "            print(\"OpenAI API key is not set. Cannot generate with OpenAI.\")\n",
        "            return \"OpenAI API key is not configured.\"\n",
        "\n",
        "        response = openai.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=500,\n",
        "            temperature=0.7\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except openai.APIError as e:\n",
        "        print(f\"Erreur OpenAI API: {e}\")\n",
        "        return \"Désolé, une erreur est survenue lors de la communication avec OpenAI.\"\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur inattendue avec OpenAI: {e}\")\n",
        "        return \"Désolé, une erreur inattendue est survenue.\"\n",
        "\n",
        "def generate_with_gemini(prompt: str, model: str = \"gemini-pro\") -> str:\n",
        "    \"\"\"Génère une réponse en utilisant l'API Gemini.\"\"\"\n",
        "    try:\n",
        "        # Ensure genai is configured\n",
        "        if not genai.get_client():\n",
        "             print(\"Gemini API is not configured. Cannot generate with Gemini.\")\n",
        "             return \"Gemini API is not configured.\"\n",
        "\n",
        "        model_gemini = genai.GenerativeModel(model)\n",
        "        response = model_gemini.generate_content(prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur Gemini API: {e}\")\n",
        "        return \"Désolé, une erreur est survenue lors de la communication avec Gemini.\"\n",
        "\n",
        "# --- 3. Fonction Principale du RAG Modifiée ---\n",
        "\n",
        "def run_rag_system(query: str, use_openai: bool = True, use_gemini: bool = False) -> str:\n",
        "    \"\"\"\n",
        "    Exécute le système RAG en tenant compte des liens de la mind map.\n",
        "    \"\"\"\n",
        "    print(f\"\\nRequête de l'utilisateur: {query}\")\n",
        "\n",
        "    # Étape 1: Récupération de la Mind Map (texte et liens)\n",
        "    print(\"Extraction des informations textuelles et des liens pertinents de la mind map...\")\n",
        "    mindmap_text_info, linked_doc_ids = extract_info_and_links_from_mindmap(query, mindmap_data)\n",
        "\n",
        "    if mindmap_text_info:\n",
        "        print(f\"Informations textuelles de la mind map récupérées ({len(mindmap_text_info)}):\")\n",
        "        for i, info in enumerate(mindmap_text_info):\n",
        "            print(f\"- Info {i+1}: {info}\")\n",
        "    else:\n",
        "        print(\"Aucune information textuelle pertinente trouvée dans la mind map.\")\n",
        "\n",
        "    if linked_doc_ids:\n",
        "        print(f\"IDs de documents liés trouvés dans la mind map ({len(linked_doc_ids)}): {linked_doc_ids}\")\n",
        "    else:\n",
        "        print(\"Aucun document lié directement trouvé dans la mind map pour cette requête.\")\n",
        "\n",
        "    # Étape 2: Récupération des Documents\n",
        "    # D'abord, récupérer les documents spécifiquement liés par la mind map\n",
        "    retrieved_docs_linked = []\n",
        "    if linked_doc_ids:\n",
        "        print(\"Récupération des documents liés via la mind map...\")\n",
        "        # Pour les documents liés, on peut récupérer plus de chunks pour s'assurer d'avoir le contexte\n",
        "        retrieved_docs_linked = retrieve_from_chromadb(query, top_k=5, document_ids_filter=linked_doc_ids)\n",
        "        if retrieved_docs_linked:\n",
        "            print(f\"Documents liés récupérés ({len(retrieved_docs_linked)}):\")\n",
        "            for i, doc in enumerate(retrieved_docs_linked):\n",
        "                print(f\"- Doc lié {i+1}: {doc[:100]}...\")\n",
        "        else:\n",
        "            print(\"Aucun chunk pertinent trouvé pour les documents liés.\")\n",
        "\n",
        "    # Ensuite, récupérer des documents génériques si les liés ne sont pas suffisants,\n",
        "    # ou si aucun lien n'a été trouvé.\n",
        "    # Assurez-vous de ne pas récupérer les mêmes documents si déjà liés.\n",
        "    retrieved_docs_generic = []\n",
        "    # Only retrieve generic docs if linked docs are insufficient or none were found AND the ChromaDB collection was loaded\n",
        "    if collection is not None and (not retrieved_docs_linked or len(retrieved_docs_linked) < 3): # Exemple: si moins de 3 documents liés\n",
        "        print(\"Récupération de documents génériques supplémentaires de ChromaDB...\")\n",
        "        # On peut exclure les IDs déjà traités si nécessaire (nécessite une petite amélioration de retrieve_from_chromadb)\n",
        "        # Pour l'instant, on les ajoute simplement. La déduplication sera faite implicitement par le LLM.\n",
        "        retrieved_docs_generic = retrieve_from_chromadb(query, top_k=5) # document_ids_filter=None here\n",
        "        if retrieved_docs_generic:\n",
        "            print(f\"Documents génériques récupérés ({len(retrieved_docs_generic)}):\")\n",
        "            for i, doc in enumerate(retrieved_docs_generic):\n",
        "                print(f\"- Doc générique {i+1}: {doc[:100]}...\")\n",
        "        else:\n",
        "            print(\"Aucun document générique pertinent trouvé dans ChromaDB.\")\n",
        "\n",
        "\n",
        "    # Combiner tous les documents récupérés (liés + génériques)\n",
        "    # Filter out None values from the lists before combining\n",
        "    all_retrieved_docs = list(set([doc for doc in retrieved_docs_linked if doc is not None] + [doc for doc in retrieved_docs_generic if doc is not None])) # Utilise set pour dédupliquer\n",
        "    if not all_retrieved_docs:\n",
        "        print(\"Aucun document pertinent (lié ou générique) trouvé au total.\")\n",
        "\n",
        "    # Étape 3: Augmentation\n",
        "    rag_prompt = build_rag_prompt(query, all_retrieved_docs, mindmap_text_info)\n",
        "    print(\"\\n--- Prompt généré pour le LLM ---\")\n",
        "    print(rag_prompt)\n",
        "    print(\"---------------------------------\\n\")\n",
        "\n",
        "    # Étape 4: Génération\n",
        "    response = \"Aucun modèle sélectionné pour la génération.\"\n",
        "\n",
        "    if use_openai:\n",
        "        print(\"Génération de la réponse avec OpenAI...\")\n",
        "        response = generate_with_openai(rag_prompt)\n",
        "        print(\"\\n--- Réponse d'OpenAI ---\")\n",
        "        print(response)\n",
        "        print(\"------------------------\\n\")\n",
        "    elif use_gemini:\n",
        "        print(\"Génération de la réponse avec Gemini...\")\n",
        "        response = generate_with_gemini(rag_prompt)\n",
        "        print(\"\\n--- Réponse de Gemini ---\")\n",
        "        print(response)\n",
        "        print(\"-------------------------\\n\")\n",
        "\n",
        "    return response\n",
        "\n",
        "# --- Exemple d'utilisation (avec données de test mises à jour) ---\n",
        "\n",
        "# Define the directory to store the ChromaDB database for the Langchain Chroma instance\n",
        "# This should match the directory used in the embedding script\n",
        "langchain_chroma_persist_directory = '/content/sample_data/MM_RAG/db'\n",
        "\n",
        "# Placeholder for embeddings. Ensure embeddings are initialized before this block\n",
        "# embeddings = GoogleGenerativeAIEmbeddings(...) # This should be done in a previous cell\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Création/Mise à jour du fichier de mind map test ---\n",
        "    # Adaptez cette structure à votre JSON de mind map réel\n",
        "    # Assurez-vous que les 'linked_docs' correspondent aux 'document_id' de vos chunks ChromaDB\n",
        "    # Note: The 'linked_docs' here should ideally match the 'source' metadata used in ChromaDB\n",
        "    # during ingestion, if you are using 'source' for filtering.\n",
        "    mindmap_test_data = {\n",
        "        \"nodes\": [\n",
        "            {\"id\": \"1\", \"text\": \"Projet RAG\", \"details\": \"Développement d'un système RAG. Voir document sur l'architecture.\", \"linked_docs\": [\"/content/sample_data/MM_RAG/Documents/architecture.pdf\"]}, # Example linked doc path\n",
        "            {\"id\": \"2\", \"text\": \"Mind Map\", \"details\": \"Transformée en JSON, contient des liens vers des spécifications.\", \"linked_docs\": [\"/content/sample_data/MM_RAG/Documents/specifications.pdf\"]}, # Example linked doc path\n",
        "            {\"id\": \"3\", \"text\": \"Documents\", \"details\": \"Décomposés en chunks et stockés dans ChromaDB.\", \"linked_docs\": []},\n",
        "            {\"id\": \"4\", \"text\": \"LLMs et Intégration\", \"details\": \"Utilisation d'OpenAI et Gemini pour la génération. Document sur les APIs.\", \"linked_docs\": [\"/content/sample_data/MM_RAG/Documents/apis.pdf\"]}, # Example linked doc path\n",
        "            {\"id\": \"5\", \"text\": \"Base de Données\", \"details\": \"ChromaDB pour les embeddings et la récupération vectorielle.\", \"linked_docs\": [\"/content/sample_data/MM_RAG/Documents/database.pdf\"]}, # Example linked doc path\n",
        "            {\"id\": \"6\", \"text\": \"Déploiement\", \"details\": \"Guide de déploiement.\", \"linked_docs\": [\"/content/sample_data/MM_RAG/Documents/deployment_guide.pdf\"]} # Example linked doc path\n",
        "        ],\n",
        "        \"edges\": []\n",
        "    }\n",
        "    # Ensure the directory for the mind map JSON exists if it's different from /content/sample_data/MM_RAG\n",
        "    os.makedirs(os.path.dirname(MINDMAP_JSON_PATH), exist_ok=True)\n",
        "    with open(MINDMAP_JSON_PATH, 'w', encoding='utf-8') as f:\n",
        "        json.dump(mindmap_test_data, f, indent=2)\n",
        "    print(f\"Fichier de mind map test créé/mis à jour à {MINDMAP_JSON_PATH}\")\n",
        "\n",
        "    # Recharger les données de la mind map after creating the file\n",
        "    # Read the JSON file into the mindmap_data variable\n",
        "    try:\n",
        "        with open(MINDMAP_JSON_PATH, 'r', encoding='utf-8') as f:\n",
        "            mindmap_data = json.load(f)\n",
        "        print(f\"Mind map data reloaded from {MINDMAP_JSON_PATH}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Mind map file not found at {MINDMAP_JSON_PATH} after creation attempt.\")\n",
        "        mindmap_data = {} # Reset to empty if file not found\n",
        "    except json.JSONDecodeError:\n",
        "         print(f\"Error: Could not decode JSON from {MINDMAP_JSON_PATH} after creation attempt.\")\n",
        "         mindmap_data = {} # Reset to empty if JSON is invalid\n",
        "\n",
        "    # Load the ChromaDB vector store using Langchain's Chroma class\n",
        "    # This is separate from the chromadb client used for collection-based retrieval\n",
        "    # Ensure the embeddings function is the same as the one used for saving\n",
        "    try:\n",
        "        loaded_db = Chroma(persist_directory=langchain_chroma_persist_directory, embedding_function=embeddings)\n",
        "        print(f\"ChromaDB database loaded successfully from {langchain_chroma_persist_directory}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading ChromaDB database from {langchain_chroma_persist_directory}: {e}\")\n",
        "        print(\"Please ensure the directory exists and contains a valid ChromaDB database.\")\n",
        "        loaded_db = None # Set to None if loading fails\n",
        "\n",
        "\n",
        "    # Exemples de requêtes\n",
        "    print(\"\\n--- Test RAG avec liens de la mind map (OpenAI) ---\")\n",
        "    query1 = \"quelles sont les soft-skills pour continuer les activités de cet experts pour le nouvel arrivants\"\n",
        "    run_rag_system(query1, use_openai=True, use_gemini=False)\n",
        "\n",
        "    print(\"\\n--- Test RAG avec liens de la mind map (Gemini) ---\")\n",
        "    query2 = \"Quels sont les projets sur lequel le collaboraterur a travailler??\"\n",
        "    run_rag_system(query2, use_openai=False, use_gemini=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGbswmpg3MMW",
        "outputId": "4e514517-0bf3-409e-f614-e06fc1b10125"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Erreur lors du chargement de la collection ChromaDB '/content/sample_data/MM_RAG/Documents': Collection [/content/sample_data/MM_RAG/Documents] does not exists\n",
            "Assurez-vous que la collection existe et que le chemin est correct.\n",
            "Mind map chargée depuis /content/sample_data/MM_RAG/off_boarding_MM.json\n",
            "Fichier de mind map test créé/mis à jour à /content/sample_data/MM_RAG/off_boarding_MM.json\n",
            "Mind map data reloaded from /content/sample_data/MM_RAG/off_boarding_MM.json\n",
            "Error loading ChromaDB database from /content/sample_data/MM_RAG/db: name 'embeddings' is not defined\n",
            "Please ensure the directory exists and contains a valid ChromaDB database.\n",
            "\n",
            "--- Test RAG avec liens de la mind map (OpenAI) ---\n",
            "\n",
            "Requête de l'utilisateur: quelles sont les soft-skills pour continuer les activités de cet experts pour le nouvel arrivants\n",
            "Extraction des informations textuelles et des liens pertinents de la mind map...\n",
            "Aucune information textuelle pertinente trouvée dans la mind map.\n",
            "Aucun document lié directement trouvé dans la mind map pour cette requête.\n",
            "Aucun document pertinent (lié ou générique) trouvé au total.\n",
            "\n",
            "--- Prompt généré pour le LLM ---\n",
            "Vous êtes un assistant IA qui fournit des réponses basées sur les informations fournies.\n",
            "Voici la question de l'utilisateur:\n",
            "Question: quelles sont les soft-skills pour continuer les activités de cet experts pour le nouvel arrivants\n",
            "\n",
            "En utilisant uniquement les informations fournies (extraits de documents et de la mind map), répondez à la question de l'utilisateur de manière complète et concise. Si les informations ne sont pas suffisantes, indiquez-le.\n",
            "---------------------------------\n",
            "\n",
            "Génération de la réponse avec OpenAI...\n",
            "\n",
            "--- Réponse d'OpenAI ---\n",
            "Les soft-skills nécessaires pour continuer les activités de cet expert pour les nouveaux arrivants sont:\n",
            "\n",
            "1. Empathie\n",
            "2. Communication efficace\n",
            "3. Adaptabilité\n",
            "4. Patience\n",
            "5. Capacité d'écoute active\n",
            "6. Capacité à travailler en équipe\n",
            "7. Capacité de résolution de problèmes\n",
            "\n",
            "Ces soft-skills sont essentiels pour accompagner et intégrer efficacement les nouveaux arrivants dans leurs activités.\n",
            "------------------------\n",
            "\n",
            "\n",
            "--- Test RAG avec liens de la mind map (Gemini) ---\n",
            "\n",
            "Requête de l'utilisateur: Quels sont les projets sur lequel le collaboraterur a travailler??\n",
            "Extraction des informations textuelles et des liens pertinents de la mind map...\n",
            "Aucune information textuelle pertinente trouvée dans la mind map.\n",
            "Aucun document lié directement trouvé dans la mind map pour cette requête.\n",
            "Aucun document pertinent (lié ou générique) trouvé au total.\n",
            "\n",
            "--- Prompt généré pour le LLM ---\n",
            "Vous êtes un assistant IA qui fournit des réponses basées sur les informations fournies.\n",
            "Voici la question de l'utilisateur:\n",
            "Question: Quels sont les projets sur lequel le collaboraterur a travailler??\n",
            "\n",
            "En utilisant uniquement les informations fournies (extraits de documents et de la mind map), répondez à la question de l'utilisateur de manière complète et concise. Si les informations ne sont pas suffisantes, indiquez-le.\n",
            "---------------------------------\n",
            "\n",
            "Génération de la réponse avec Gemini...\n",
            "Erreur Gemini API: module 'google.generativeai' has no attribute 'get_client'\n",
            "\n",
            "--- Réponse de Gemini ---\n",
            "Désolé, une erreur est survenue lors de la communication avec Gemini.\n",
            "-------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2H0K2YAI7eK_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xz2Qz3A0-OrG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xdJUy3QMy8bP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector Database Indexing\n",
        "All generated embeddings (from mind map nodes and document chunks) are indexed in a vector database. For this work, we utilize Vertex AI Vector Search (formerly Matching Engine), which provides highly scalable and efficient nearest-neighbor search capabilities, seamlessly integrated within the Google Cloud ecosystem. Each indexed vector is associated with metadata, including:\n",
        "\n",
        "For mind map nodes: node_id, title, description, related_document_ids.\n",
        "\n",
        "For document chunks: doc_id, chunk_index, original_text.\n",
        "This metadata is crucial for the retrieval phase.\n",
        "\n",
        "\n",
        "**My database is named DB**\n",
        "\n",
        "> Ajouter une citation\n",
        "\n"
      ],
      "metadata": {
        "id": "xwOxM5pCnOHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q/A Mindmpad + documents\n"
      ],
      "metadata": {
        "id": "pTVVgFKQEOpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain langchain-google-genai google-generativeai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "p_BHyHoHFclO",
        "outputId": "a5e3efe0-6e2a-43b9-c46e-5a0cea03174d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.11/dist-packages (2.0.10)\n",
            "Collecting langchain-google-genai\n",
            "  Using cached langchain_google_genai-2.1.5-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.65)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.5)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain-google-genai)\n",
            "  Using cached google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "INFO: pip is looking at multiple versions of google-generativeai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting google-generativeai\n",
            "  Using cached google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Using cached google_generativeai-0.8.4-py3-none-any.whl.metadata (4.2 kB)\n",
            "  Using cached google_generativeai-0.8.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Using cached google_generativeai-0.8.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "INFO: pip is still looking at multiple versions of google-generativeai to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached google_generativeai-0.8.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Using cached google_generativeai-0.8.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached google_generativeai-0.7.2-py3-none-any.whl.metadata (4.0 kB)\n",
            "  Using cached google_generativeai-0.7.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Using cached google_generativeai-0.7.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Using cached google_generativeai-0.6.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Using cached google_generativeai-0.5.4-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Using cached google_generativeai-0.5.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Using cached google_generativeai-0.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Using cached google_generativeai-0.5.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Using cached google_generativeai-0.5.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Using cached google_generativeai-0.4.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "  Using cached google_generativeai-0.4.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "  Using cached google_generativeai-0.3.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached google_generativeai-0.3.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached google_generativeai-0.3.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Using cached google_generativeai-0.2.2-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Using cached google_generativeai-0.2.1-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Using cached google_generativeai-0.2.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Using cached google_generativeai-0.1.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-google-genai\n",
            "  Using cached langchain_google_genai-2.1.4-py3-none-any.whl.metadata (5.2 kB)\n",
            "  Using cached langchain_google_genai-2.1.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Using cached langchain_google_genai-2.1.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Using cached langchain_google_genai-2.1.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Using cached langchain_google_genai-2.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Using cached langchain_google_genai-2.0.11-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.171.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.14.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.72.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: un code qui me permet d'intérroger mes documents et my mindmap ( equi est dans la base de vecteurs) pour un retrait des information et ce que je dois modficier dans le détail\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# --- Instructions for querying ---\n",
        "\n",
        "# 1. Load the Vector Database:\n",
        "#    - Ensure the `persist_directory` variable below matches the directory where you saved your ChromaDB database.\n",
        "#    - Ensure `embeddings` are loaded with the same configuration as when you created the database.\n",
        "\n",
        "# 2. Configure the Language Model:\n",
        "#    - We will use a Google Generative AI model for answering questions.\n",
        "#    - Replace \"gemini-pro\" with the desired model name if needed.\n",
        "\n",
        "# 3. Ask Questions:\n",
        "#    - Replace the example `query` string with the question you want to ask your documents and mind map.\n",
        "\n",
        "# --- Load the Vector Database ---\n",
        "\n",
        "# Define the directory where the ChromaDB database is stored\n",
        "persist_directory = '/content/sample_data/MM_RAG/db' # <--- Ensure this matches the directory where you saved your database\n",
        "\n",
        "# Load the ChromaDB vector store\n",
        "# Make sure the embeddings function is the same as the one used for saving\n",
        "try:\n",
        "    loaded_db = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
        "    print(f\"ChromaDB database loaded successfully from {persist_directory}.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading ChromaDB database: {e}\")\n",
        "    print(\"Please ensure the 'db' directory exists and contains a valid ChromaDB database.\")\n",
        "    loaded_db = None # Set to None if loading fails\n",
        "\n",
        "# --- Configure the Language Model ---\n",
        "api_key = 'AIzaSyBOZYhKopjATzjjhUrSMjj1iq7wJGww8OE'\n",
        "# Initialize the Google Generative AI model for chat\n",
        "# Ensure you have your API key configured correctly as done previously\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=api_key)\n",
        "\n",
        "# --- Create the RetrievalQA Chain ---\n",
        "\n",
        "# If the database loaded successfully, create the RetrievalQA chain\n",
        "qa_chain = None\n",
        "if loaded_db:\n",
        "    # Create a retriever from the loaded database\n",
        "    # This retriever will search for relevant document chunks based on the query\n",
        "    retriever = loaded_db.as_retriever()\n",
        "\n",
        "    # Create the RetrievalQA chain\n",
        "    # This chain combines the retriever (for finding relevant documents) and the LLM (for answering the question based on the documents)\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",  # \"stuff\" means it will stuff all retrieved documents into the LLM context\n",
        "        retriever=retriever,\n",
        "        return_source_documents=True # Set to True to see which documents were used to answer\n",
        "    )\n",
        "    print(\"RetrievalQA chain created.\")\n",
        "else:\n",
        "    print(\"Cannot create RetrievalQA chain as the database failed to load.\")\n",
        "\n",
        "\n",
        "# --- Ask a Question ---\n",
        "\n",
        "# Define your question here\n",
        "query = \"quelles sont les soft-skills pour continuer les activités de cet experts pour le nouvel arrivants \" # <--- Put your question here\n",
        "#quelles sont les soft-skills pour continuer les activités de cet experts pour le nouvel arrivants\n",
        "# Run the QA chain with your query\n",
        "if qa_chain:\n",
        "    print(f\"\\nAsking the question: '{query}'\")\n",
        "    try:\n",
        "        result = qa_chain.invoke({\"query\": query})\n",
        "\n",
        "        # --- Display the Results ---\n",
        "        print(\"\\n--- Answer ---\")\n",
        "        print(result['result'])\n",
        "\n",
        "        print(\"\\n--- Sources ---\")\n",
        "        if result['source_documents']:\n",
        "            for doc in result['source_documents']:\n",
        "                print(f\"Source: {doc.metadata.get('source', 'N/A')}, Type: {doc.metadata.get('type', 'N/A')}\")\n",
        "                print(f\"Content Preview: {doc.page_content[:150]}...\") # Show a preview of the content\n",
        "                print(\"-\" * 10)\n",
        "        else:\n",
        "            print(\"No source documents found for this query.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while querying the chain: {e}\")\n",
        "else:\n",
        "    print(\"Cannot process query as the QA chain was not created.\")\n",
        "\n",
        "# --- What you might need to modify in detail ---\n",
        "\n",
        "# 1. Data Parsing Functions (`parse_xmind_json`, `parse_pdf`):\n",
        "#    - **Detailed XMind Parsing:** The current `parse_xmind_json` function is very basic (only title and first level topics). You will likely need to modify this function to extract more granular information from your XMind JSON, such as:\n",
        "#      - Subtopics at deeper levels\n",
        "#      - Notes associated with topics\n",
        "#      - Markers or labels\n",
        "#      - Relationships between topics\n",
        "#      - This requires understanding the structure of the JSON output from `xmindparser.xmind_to_dict` for your specific mind maps. You might need to recursively traverse the topic structure.\n",
        "#    - **PDF Chunking Strategy:** The current `parse_pdf` splits by double newlines. For better retrieval accuracy, you might want to implement a more advanced chunking strategy:\n",
        "#      - **Sentence splitting:** Using NLTK or SpaCy to split text into sentences before chunking.\n",
        "#      - **Fixed-size chunking with overlap:** Splitting into chunks of a specific token count with some overlap to maintain context.\n",
        "#      - **Recursive Character Text Splitter:** Langchain provides utilities like `RecursiveCharacterTextSplitter` which can be very effective.\n",
        "#    - **Metadata Extraction:** Enhance metadata extraction in both parsers (e.g., page numbers for PDFs, specific node IDs for XMind topics). This metadata can be useful for filtering or providing more context in the answer.\n",
        "\n",
        "# 2. Embedding Model and Parameters:\n",
        "#    - While `models/embedding-001` is a good starting point, you might experiment with other embedding models if available and supported by Langchain Google Generative AI, depending on the complexity and domain of your documents.\n",
        "#    - Ensure the `api_key` is correctly configured and loaded (using Colab Secrets or .env is strongly recommended over hardcoding).\n",
        "\n",
        "# 3. ChromaDB Configuration:\n",
        "#    - **Persistence Directory:** Ensure `persist_directory = 'db'` is the desired location. If you move your notebook or run it elsewhere, you'll need to make sure this path is accessible and correct.\n",
        "#    - **Collection Name:** For more advanced use cases or when handling multiple datasets, you might want to specify a collection name when initializing Chroma (e.g., `Chroma(..., collection_name=\"my_mindmap_docs\")`).\n",
        "\n",
        "# 4. Retrieval Strategy:\n",
        "#    - The current setup uses a basic `as_retriever()`. Langchain offers various retriever types:\n",
        "#      - **Multi-query retriever:** Generates multiple queries from the original query to get a broader set of relevant documents.\n",
        "#      - **Contextual compression retriever:** Uses a language model to compress the retrieved documents, keeping only the most relevant parts.\n",
        "#      - **Ensemble retriever:** Combines multiple retrievers to improve results.\n",
        "#    - You can adjust parameters for the retriever, such as `search_kwargs` (e.g., `search_kwargs={\"k\": 5}` to retrieve the top 5 most similar documents).\n",
        "\n",
        "# 5. QA Chain Type and Prompts:\n",
        "#    - `chain_type=\"stuff\"` is simple but can hit token limits for large documents. Other chain types include:\n",
        "#      - `\"map_reduce\"`: Processes chunks individually and then combines the results. Good for large documents.\n",
        "#      - `\"refine\"`: Refines an initial answer by iterating through document chunks.\n",
        "#    - **Prompts:** For more tailored responses, you can customize the prompts used by the QA chain. This involves providing instructions to the LLM on how to answer based on the retrieved documents.\n",
        "\n",
        "# 6. Error Handling and Edge Cases:\n",
        "#    - Enhance error handling for file parsing (e.g., gracefully handle corrupted files).\n",
        "#    - Consider edge cases like empty documents or mind maps.\n",
        "\n",
        "# In summary, the main areas for detailed modification to improve information retrieval will be in refining the parsing logic for both XMind JSON and PDF documents to extract more relevant and structured information, and potentially experimenting with different chunking strategies and retrieval methods to better suit the characteristics of your specific data.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ANuTP-Cs-QVx",
        "outputId": "ec378a40-cf99-42d0-8e8a-19465de6b6be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChromaDB database loaded successfully from /content/sample_data/MM_RAG/db.\n",
            "RetrievalQA chain created.\n",
            "\n",
            "Asking the question: 'quelles sont les soft-skills pour continuer les activités de cet experts pour le nouvel arrivants '\n",
            "An error occurred while querying the chain: Unknown field for Part: thought\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ... (previous imports and code) ...\n",
        "\n",
        "# --- Configure the Language Model ---\n",
        "\n",
        "# Initialize the Google Generative AI model for chat\n",
        "# Ensure you have your API key configured correctly as done previously\n",
        "# Assuming 'api_key' is defined and configured in a previous cell\n",
        "try:\n",
        "    import google.generativeai as genai\n",
        "    genai.configure(api_key=api_key)\n",
        "\n",
        "    # List available models to find one that supports 'generateContent'\n",
        "    print(\"\\nListing available models:\")\n",
        "    available_models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]\n",
        "    print(f\"Available models supporting generateContent: {available_models}\")\n",
        "\n",
        "    # --- IMPORTANT: Choose a suitable model from the available list ---\n",
        "    # Look at the output above (the list of available models).\n",
        "    # Replace 'gemini-pro' in the line below with one of the model names\n",
        "    # from the 'Available models supporting generateContent' list.\n",
        "    # Common alternatives include 'gemini-1.0-pro' or 'gemini-1.5-pro-latest'.\n",
        "    model_name = \"gemini-pro\" # <--- ***CHANGE THIS LINE*** based on the list printed above\n",
        "\n",
        "    if model_name not in available_models:\n",
        "        print(f\"Error: The chosen model '{model_name}' is not in the list of available models supporting 'generateContent'.\")\n",
        "        if available_models:\n",
        "             print(f\"Please choose a model from this list: {available_models}\")\n",
        "        else:\n",
        "             print(\"No models supporting 'generateContent' found.\")\n",
        "        llm = None # Set llm to None to prevent trying to use a non-existent model\n",
        "    else:\n",
        "        llm = ChatGoogleGenerativeAI(model=model_name, google_api_key=api_key)\n",
        "        print(f\"Using language model: {model_name}\")\n",
        "\n",
        "except NameError:\n",
        "    print(\"Error: 'api_key' is not defined. Please run the cell that sets the API key first.\")\n",
        "    llm = None\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while configuring the language model: {e}\")\n",
        "    llm = None\n",
        "\n",
        "# ... (rest of the code) ..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "XQf0g9tIB3G1",
        "outputId": "68187949-c5ae-419d-e40a-88e97983ae04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Listing available models:\n",
            "Available models supporting generateContent: ['models/gemini-1.0-pro-vision-latest', 'models/gemini-pro-vision', 'models/gemini-1.5-pro-latest', 'models/gemini-1.5-pro-002', 'models/gemini-1.5-pro', 'models/gemini-1.5-flash-latest', 'models/gemini-1.5-flash', 'models/gemini-1.5-flash-002', 'models/gemini-1.5-flash-8b', 'models/gemini-1.5-flash-8b-001', 'models/gemini-1.5-flash-8b-latest', 'models/gemini-2.5-pro-exp-03-25', 'models/gemini-2.5-pro-preview-03-25', 'models/gemini-2.5-flash-preview-04-17', 'models/gemini-2.5-flash-preview-05-20', 'models/gemini-2.5-flash-preview-04-17-thinking', 'models/gemini-2.5-pro-preview-05-06', 'models/gemini-2.5-pro-preview-06-05', 'models/gemini-2.0-flash-exp', 'models/gemini-2.0-flash', 'models/gemini-2.0-flash-001', 'models/gemini-2.0-flash-lite-001', 'models/gemini-2.0-flash-lite', 'models/gemini-2.0-flash-lite-preview-02-05', 'models/gemini-2.0-flash-lite-preview', 'models/gemini-2.0-pro-exp', 'models/gemini-2.0-pro-exp-02-05', 'models/gemini-exp-1206', 'models/gemini-2.0-flash-thinking-exp-01-21', 'models/gemini-2.0-flash-thinking-exp', 'models/gemini-2.0-flash-thinking-exp-1219', 'models/gemini-2.5-flash-preview-tts', 'models/gemini-2.5-pro-preview-tts', 'models/learnlm-2.0-flash-experimental', 'models/gemma-3-1b-it', 'models/gemma-3-4b-it', 'models/gemma-3-12b-it', 'models/gemma-3-27b-it', 'models/gemma-3n-e4b-it']\n",
            "Error: The chosen model 'gemini-pro' is not in the list of available models supporting 'generateContent'.\n",
            "Please choose a model from this list: ['models/gemini-1.0-pro-vision-latest', 'models/gemini-pro-vision', 'models/gemini-1.5-pro-latest', 'models/gemini-1.5-pro-002', 'models/gemini-1.5-pro', 'models/gemini-1.5-flash-latest', 'models/gemini-1.5-flash', 'models/gemini-1.5-flash-002', 'models/gemini-1.5-flash-8b', 'models/gemini-1.5-flash-8b-001', 'models/gemini-1.5-flash-8b-latest', 'models/gemini-2.5-pro-exp-03-25', 'models/gemini-2.5-pro-preview-03-25', 'models/gemini-2.5-flash-preview-04-17', 'models/gemini-2.5-flash-preview-05-20', 'models/gemini-2.5-flash-preview-04-17-thinking', 'models/gemini-2.5-pro-preview-05-06', 'models/gemini-2.5-pro-preview-06-05', 'models/gemini-2.0-flash-exp', 'models/gemini-2.0-flash', 'models/gemini-2.0-flash-001', 'models/gemini-2.0-flash-lite-001', 'models/gemini-2.0-flash-lite', 'models/gemini-2.0-flash-lite-preview-02-05', 'models/gemini-2.0-flash-lite-preview', 'models/gemini-2.0-pro-exp', 'models/gemini-2.0-pro-exp-02-05', 'models/gemini-exp-1206', 'models/gemini-2.0-flash-thinking-exp-01-21', 'models/gemini-2.0-flash-thinking-exp', 'models/gemini-2.0-flash-thinking-exp-1219', 'models/gemini-2.5-flash-preview-tts', 'models/gemini-2.5-pro-preview-tts', 'models/learnlm-2.0-flash-experimental', 'models/gemma-3-1b-it', 'models/gemma-3-4b-it', 'models/gemma-3-12b-it', 'models/gemma-3-27b-it', 'models/gemma-3n-e4b-it']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.4. Hybrid Retrieval Strategy**\n",
        "The core novelty of MM-RAG lies in its hybrid retrieval strategy, which leverages the mind map's structure to guide and refine the document retrieval process. This two-stage approach ensures that the context provided to the LLM is not only semantically relevant but also structurally grounded within the organizational knowledge framework.\n",
        "\n",
        "A. Stage 1: Mind Map Node Retrieval:\n",
        "When a user poses a query, its embedding is first generated using models/embedding-001 with task_type=\"RETRIEVAL_QUERY\". This query embedding is then used to perform a similarity search against the indexed mind map node embeddings in Vertex AI Vector Search. The top-K most similar mind map nodes are retrieved. This step effectively identifies the most relevant conceptual areas within the offboarding knowledge base."
      ],
      "metadata": {
        "id": "5FbrK_WEnZRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_mind_map_nodes(query_embedding, vector_db_client, k=5):\n",
        "    \"\"\"Retrieves top-K mind map nodes similar to the query.\"\"\"\n",
        "    # This is a conceptual call; actual implementation depends on Vertex AI Vector Search API\n",
        "    results = vector_db_client.query(query_embedding, index_name=\"mind_map_nodes_index\", top_k=k)\n",
        "    return results # Returns list of {node_id, score, metadata}"
      ],
      "metadata": {
        "id": "94kB8uXGnS8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "B. Stage 2: Contextualized Document Chunk Retrieval:"
      ],
      "metadata": {
        "id": "i36cNm8CngbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_document_chunks(query_embedding, vector_db_client, relevant_doc_ids, m=10):\n",
        "    \"\"\"Retrieves top-M document chunks from a filtered set of documents.\"\"\"\n",
        "    # Conceptual call; actual implementation would filter by doc_id metadata\n",
        "    # Some vector DBs allow direct filtering during query, others require post-filtering\n",
        "    results = vector_db_client.query(\n",
        "        query_embedding,\n",
        "        index_name=\"document_chunks_index\",\n",
        "        top_k=m,\n",
        "        filter_by_metadata={'doc_id': relevant_doc_ids}\n",
        "    )\n",
        "    return results # Returns list of {doc_id, chunk_index, text, score}\n",
        "\n",
        "def get_augmented_context(user_query, mind_map_nodes_data, doc_chunks_data):\n",
        "    query_embedding = get_gemini_embedding(user_query, task_type=\"RETRIEVAL_QUERY\")\n",
        "\n",
        "    # Stage 1: Retrieve mind map nodes\n",
        "    retrieved_nodes_info = retrieve_mind_map_nodes(query_embedding, vector_db_client, k=5)\n",
        "\n",
        "    relevant_doc_ids = set()\n",
        "    mind_map_context_str = \"Relevant Mind Map Concepts:\\n\"\n",
        "    for node_info in retrieved_nodes_info:\n",
        "        node_id = node_info['node_id']\n",
        "        node_metadata = mind_map_nodes_embeddings[node_id] # Assuming we store metadata in a dict\n",
        "        mind_map_context_str += f\"- {node_metadata['title']}: {node_metadata['description']}\\n\"\n",
        "        relevant_doc_ids.update(node_metadata['related_document_ids'])\n",
        "\n",
        "    # Stage 2: Retrieve document chunks based on relevant documents\n",
        "    retrieved_chunks_info = retrieve_document_chunks(query_embedding, vector_db_client, list(relevant_doc_ids), m=10)\n",
        "\n",
        "    document_context_str = \"Relevant Document Snippets:\\n\"\n",
        "    for chunk_info in retrieved_chunks_info:\n",
        "        document_context_str += f\"- {chunk_info['text']}\\n\" # Assuming 'text' is part of retrieved info\n",
        "\n",
        "    return mind_map_context_str + \"\\n\" + document_context_str"
      ],
      "metadata": {
        "id": "FY5ezpvWndIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Augmented Generation with Gemini Pro\n",
        "\n",
        "\n",
        "The final stage involves leveraging Gemini Pro to synthesize a comprehensive and accurate response. The retrieved context, comprising both the mind map node descriptions and the relevant document chunks, is concatenated and passed to Gemini Pro along with the user's original query.\n",
        "\n",
        "The prompt engineering for Gemini Pro is crucial. We instruct the model to:\n",
        "\n",
        "Answer the question based only on the provided context.\n",
        "\n",
        "Clearly state if the information is insufficient to answer the question.\n",
        "\n",
        "Structure the answer clearly (e.g., bullet points, short paragraphs).\n",
        "\n",
        "Prioritize factual accuracy and avoid speculation.\n"
      ],
      "metadata": {
        "id": "t-le_O1znmUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "# genai.configure(api_key=\"YOUR_API_KEY\") # Ensure API key is configured\n",
        "\n",
        "def generate_response(user_query, augmented_context):\n",
        "    \"\"\"Generates a response using Gemini Pro based on the query and augmented context.\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    You are an intelligent knowledge transfer assistant.\n",
        "    Based *only* on the following context, answer the user's question.\n",
        "    If the context does not contain enough information to answer the question, state that clearly.\n",
        "\n",
        "    ---\n",
        "    Context:\n",
        "    {augmented_context}\n",
        "    ---\n",
        "\n",
        "    User Question: {user_query}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "\n",
        "    model = genai.GenerativeModel('gemini-pro')\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating content with Gemini Pro: {e}\")\n",
        "        return \"An error occurred while generating the response.\""
      ],
      "metadata": {
        "id": "JGG9nqYynuK1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}