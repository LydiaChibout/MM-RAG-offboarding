{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzm4U-TgmQ_u"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "# genai.configure(api_key=\"YOUR_API_KEY\") # Configure your Gemini API key\n",
        "\n",
        "def get_gemini_embedding(text, task_type=\"RETRIEVAL_DOCUMENT\"):\n",
        "    \"\"\"Generates an embedding for the given text using Gemini's embedding model.\"\"\"\n",
        "    try:\n",
        "        response = genai.embed_content(\n",
        "            model=\"models/embedding-001\",\n",
        "            content=text,\n",
        "            task_type=task_type\n",
        "        )\n",
        "        return response['embedding']\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating embedding: {e}\")\n",
        "        return None\n",
        "\n",
        "mind_map_nodes_embeddings = {}\n",
        "# Assume 'mind_map_data' is loaded from your JSON mind map file\n",
        "for node_id, node_data in mind_map_data['nodes'].items():\n",
        "    node_text = f\"{node_data['title']}. {node_data['description']}\"\n",
        "\n",
        "    # Add parent context\n",
        "    if 'parent_id' in node_data and node_data['parent_id'] in mind_map_data['nodes']:\n",
        "        node_text += f\" Parent: {mind_map_data['nodes'][node_data['parent_id']]['title']}.\"\n",
        "\n",
        "    # Add children context\n",
        "    if 'child_ids' in node_data:\n",
        "        children_titles = [mind_map_data['nodes'][cid]['title'] for cid in mind_map_data['nodes'] if cid in node_data['child_ids']]\n",
        "        if children_titles:\n",
        "            node_text += f\" Children: {', '.join(children_titles)}.\"\n",
        "\n",
        "    embedding = get_gemini_embedding(node_text, task_type=\"RETRIEVAL_DOCUMENT\")\n",
        "    if embedding:\n",
        "        mind_map_nodes_embeddings[node_id] = {\n",
        "            \"text\": node_text, # Store for debugging/inspection\n",
        "            \"embedding\": embedding,\n",
        "            \"title\": node_data['title'],\n",
        "            \"description\": node_data['description'],\n",
        "            \"related_document_ids\": node_data.get('related_document_ids', [])\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gemini Embeddings for Document Chunks\n",
        "\n",
        "Similarly, each preprocessed document chunk is converted into a vector embedding using the same models/embedding-001 with task_type=\"RETRIEVAL_DOCUMENT\". This ensures semantic compatibility between node embeddings and document chunk embeddings."
      ],
      "metadata": {
        "id": "nYjzuuClmzfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_chunks_embeddings = []\n",
        "# Assume 'processed_documents' is a dictionary {doc_id: [chunk_text1, chunk_text2, ...]}\n",
        "for doc_id, chunks in processed_documents.items():\n",
        "    for i, chunk_text in enumerate(chunks):\n",
        "        embedding = get_gemini_embedding(chunk_text, task_type=\"RETRIEVAL_DOCUMENT\")\n",
        "        if embedding:\n",
        "            document_chunks_embeddings.append({\n",
        "                \"doc_id\": doc_id,\n",
        "                \"chunk_index\": i,\n",
        "                \"text\": chunk_text,\n",
        "                \"embedding\": embedding\n",
        "            })\n"
      ],
      "metadata": {
        "id": "2GPzu2o8mxXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z0DXPVKLm-tJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mxiYEQTCnMkS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector Database Indexing\n",
        "All generated embeddings (from mind map nodes and document chunks) are indexed in a vector database. For this work, we utilize Vertex AI Vector Search (formerly Matching Engine), which provides highly scalable and efficient nearest-neighbor search capabilities, seamlessly integrated within the Google Cloud ecosystem. Each indexed vector is associated with metadata, including:\n",
        "\n",
        "For mind map nodes: node_id, title, description, related_document_ids.\n",
        "\n",
        "For document chunks: doc_id, chunk_index, original_text.\n",
        "This metadata is crucial for the retrieval phase."
      ],
      "metadata": {
        "id": "xwOxM5pCnOHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.4. Hybrid Retrieval Strategy**\n",
        "The core novelty of MM-RAG lies in its hybrid retrieval strategy, which leverages the mind map's structure to guide and refine the document retrieval process. This two-stage approach ensures that the context provided to the LLM is not only semantically relevant but also structurally grounded within the organizational knowledge framework.\n",
        "\n",
        "A. Stage 1: Mind Map Node Retrieval:\n",
        "When a user poses a query, its embedding is first generated using models/embedding-001 with task_type=\"RETRIEVAL_QUERY\". This query embedding is then used to perform a similarity search against the indexed mind map node embeddings in Vertex AI Vector Search. The top-K most similar mind map nodes are retrieved. This step effectively identifies the most relevant conceptual areas within the offboarding knowledge base."
      ],
      "metadata": {
        "id": "5FbrK_WEnZRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_mind_map_nodes(query_embedding, vector_db_client, k=5):\n",
        "    \"\"\"Retrieves top-K mind map nodes similar to the query.\"\"\"\n",
        "    # This is a conceptual call; actual implementation depends on Vertex AI Vector Search API\n",
        "    results = vector_db_client.query(query_embedding, index_name=\"mind_map_nodes_index\", top_k=k)\n",
        "    return results # Returns list of {node_id, score, metadata}"
      ],
      "metadata": {
        "id": "94kB8uXGnS8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "B. Stage 2: Contextualized Document Chunk Retrieval:"
      ],
      "metadata": {
        "id": "i36cNm8CngbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_document_chunks(query_embedding, vector_db_client, relevant_doc_ids, m=10):\n",
        "    \"\"\"Retrieves top-M document chunks from a filtered set of documents.\"\"\"\n",
        "    # Conceptual call; actual implementation would filter by doc_id metadata\n",
        "    # Some vector DBs allow direct filtering during query, others require post-filtering\n",
        "    results = vector_db_client.query(\n",
        "        query_embedding,\n",
        "        index_name=\"document_chunks_index\",\n",
        "        top_k=m,\n",
        "        filter_by_metadata={'doc_id': relevant_doc_ids}\n",
        "    )\n",
        "    return results # Returns list of {doc_id, chunk_index, text, score}\n",
        "\n",
        "def get_augmented_context(user_query, mind_map_nodes_data, doc_chunks_data):\n",
        "    query_embedding = get_gemini_embedding(user_query, task_type=\"RETRIEVAL_QUERY\")\n",
        "\n",
        "    # Stage 1: Retrieve mind map nodes\n",
        "    retrieved_nodes_info = retrieve_mind_map_nodes(query_embedding, vector_db_client, k=5)\n",
        "\n",
        "    relevant_doc_ids = set()\n",
        "    mind_map_context_str = \"Relevant Mind Map Concepts:\\n\"\n",
        "    for node_info in retrieved_nodes_info:\n",
        "        node_id = node_info['node_id']\n",
        "        node_metadata = mind_map_nodes_embeddings[node_id] # Assuming we store metadata in a dict\n",
        "        mind_map_context_str += f\"- {node_metadata['title']}: {node_metadata['description']}\\n\"\n",
        "        relevant_doc_ids.update(node_metadata['related_document_ids'])\n",
        "\n",
        "    # Stage 2: Retrieve document chunks based on relevant documents\n",
        "    retrieved_chunks_info = retrieve_document_chunks(query_embedding, vector_db_client, list(relevant_doc_ids), m=10)\n",
        "\n",
        "    document_context_str = \"Relevant Document Snippets:\\n\"\n",
        "    for chunk_info in retrieved_chunks_info:\n",
        "        document_context_str += f\"- {chunk_info['text']}\\n\" # Assuming 'text' is part of retrieved info\n",
        "\n",
        "    return mind_map_context_str + \"\\n\" + document_context_str"
      ],
      "metadata": {
        "id": "FY5ezpvWndIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Augmented Generation with Gemini Pro\n",
        "\n",
        "\n",
        "The final stage involves leveraging Gemini Pro to synthesize a comprehensive and accurate response. The retrieved context, comprising both the mind map node descriptions and the relevant document chunks, is concatenated and passed to Gemini Pro along with the user's original query.\n",
        "\n",
        "The prompt engineering for Gemini Pro is crucial. We instruct the model to:\n",
        "\n",
        "Answer the question based only on the provided context.\n",
        "\n",
        "Clearly state if the information is insufficient to answer the question.\n",
        "\n",
        "Structure the answer clearly (e.g., bullet points, short paragraphs).\n",
        "\n",
        "Prioritize factual accuracy and avoid speculation.\n"
      ],
      "metadata": {
        "id": "t-le_O1znmUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "# genai.configure(api_key=\"YOUR_API_KEY\") # Ensure API key is configured\n",
        "\n",
        "def generate_response(user_query, augmented_context):\n",
        "    \"\"\"Generates a response using Gemini Pro based on the query and augmented context.\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    You are an intelligent knowledge transfer assistant.\n",
        "    Based *only* on the following context, answer the user's question.\n",
        "    If the context does not contain enough information to answer the question, state that clearly.\n",
        "\n",
        "    ---\n",
        "    Context:\n",
        "    {augmented_context}\n",
        "    ---\n",
        "\n",
        "    User Question: {user_query}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "\n",
        "    model = genai.GenerativeModel('gemini-pro')\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating content with Gemini Pro: {e}\")\n",
        "        return \"An error occurred while generating the response.\""
      ],
      "metadata": {
        "id": "JGG9nqYynuK1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}